{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "187a397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "import pickle\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, make_scorer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "def load_data(filedata):\n",
    "\n",
    "    dataframe= pd.read_csv(\n",
    "        filedata,\n",
    "        index_col=False,\n",
    "        compression=\"zip\",\n",
    "    )\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e27c7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Paso 1 ##########################\n",
    "#Limpiar los datos\n",
    "def clean_data(df):\n",
    "    df_copy = df.copy()\n",
    "    df_copy = df_copy.rename(columns={'default payment next month' : \"default\"})\n",
    "    df_copy = df_copy.drop(columns=[\"ID\"])\n",
    "    df_copy = df_copy.loc[df[\"MARRIAGE\"] != 0]\n",
    "    df_copy = df_copy.loc[df[\"EDUCATION\"] != 0]\n",
    "    df_copy[\"EDUCATION\"] = df_copy[\"EDUCATION\"].apply(lambda x: 4 if x >= 4 else x)\n",
    "    df_copy = df_copy.dropna()\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e1762d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Paso 2 ##########################\n",
    "# Divida los datasets en x_train, y_train, x_test, y_test.\n",
    "def split_data(df):\n",
    "    #X , Y\n",
    "    return df.drop(columns=[\"default\"]), df[\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49dde43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Paso 3 ##########################\n",
    "# Crear un pipeline para el modelo de clasificación\n",
    "def make_pipeline(x_train):\n",
    "    categorical_columns = [\"SEX\",\"EDUCATION\",\"MARRIAGE\"]\n",
    "    numerical_columns = list(set(x_train.columns).difference(categorical_columns))\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'), categorical_columns), \n",
    "            (\"scaler\", StandardScaler(with_mean=True, with_std=True), numerical_columns),\n",
    "        ],\n",
    "        remainder='passthrough'  # Mantener las columnas no categóricas sin transformar\n",
    "    )\n",
    "    \n",
    "    # Construcción del pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),       # Transformación de variables categóricas\n",
    "        ('pca', PCA()),                # Reducción de dimensionalidad con PCA\n",
    "        ('feature_selection', SelectKBest(score_func=f_classif)),       # Selección de características más relevantes\n",
    "        ('classifier', SVC(kernel=\"rbf\", random_state=12345, max_iter=-1))                       # Modelo final: Máquina de Soporte Vectorial\n",
    "    ])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bd1eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Paso 4 ##########################\n",
    "# Optimizar los hiperparametros del pipeline usando validación cruzada.\n",
    "def create_estimator(pipeline, x_train):\n",
    "    \n",
    "    param_grid = {\n",
    "    \"pca__n_components\": [20, x_train.shape[1] - 2],  # Probar diferentes números de componentes principales\n",
    "    'feature_selection__k': [12],            # Probar diferentes números de columnas seleccionadas\n",
    "    'classifier__kernel': [\"rbf\"],           # Ajustar el parámetro de regularización\n",
    "    'classifier__gamma': [0.1],              # Ajustes de gamma para el kernel RBF       \n",
    "    }   \n",
    "    # Definir validación cruzada con 10 splits\n",
    "    cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "    # Métrica de precisión balanceada\n",
    "    scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "    # GridSearchCV para optimización de hiperparámetros\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        scoring=scorer,\n",
    "        cv=cv,\n",
    "        n_jobs=-1  # Utilizar todos los núcleos disponibles\n",
    "    )\n",
    "\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff82ad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Paso 5 ##########################\n",
    "# Guarde el modelo (comprimido con gzip) como \"files/models/model.pkl.gz\".\n",
    "def _create_output_directory(output_directory):\n",
    "    if os.path.exists(output_directory):\n",
    "        for file in glob(f\"{output_directory}/*\"):\n",
    "            os.remove(file)\n",
    "        os.rmdir(output_directory)\n",
    "    os.makedirs(output_directory)\n",
    "    \n",
    "def _save_model(path, estimator):\n",
    "    _create_output_directory(\"files/models/\")\n",
    "\n",
    "    with gzip.open(path, \"wb\") as f:\n",
    "        pickle.dump(estimator, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14e6ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Paso 6 ##########################\n",
    "# Calcule las metricas de precision, precision balanceada, recall...\n",
    "def calculate_metrics(dataset_type, y_true, y_pred):\n",
    "    \"\"\"Calculate metrics\"\"\"\n",
    "    return {\n",
    "        \"type\": \"metrics\",\n",
    "        \"dataset\": dataset_type,\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1_score\": f1_score(y_true, y_pred, zero_division=0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42f997ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Paso 7 ##########################\n",
    "# Calcule las matrices de confusion para los conjuntos de entrenamiento y prueba\n",
    "def calculate_confusion(dataset_type, y_true, y_pred):\n",
    "    \"\"\"Confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return {\n",
    "        \"type\": \"cm_matrix\",\n",
    "        \"dataset\": dataset_type,\n",
    "        \"true_0\": {\"predicted_0\": int(cm[0][0]), \"predicted_1\": int(cm[0][1])},\n",
    "        \"true_1\": {\"predicted_0\": int(cm[1][0]), \"predicted_1\": int(cm[1][1])},\n",
    "    }\n",
    "\n",
    "def _run_jobs():\n",
    "    \n",
    "    data_train = load_data(\"./files/input/train_data.csv.zip\")\n",
    "    data_test = load_data(\"./files/input/test_data.csv.zip\")\n",
    "    data_train = clean_data(data_train)\n",
    "    data_test = clean_data(data_test)\n",
    "    x_train, y_train = split_data(data_train)\n",
    "    x_test, y_test = split_data(data_test)\n",
    "    pipeline = make_pipeline(x_train)\n",
    "\n",
    "    estimator = create_estimator(pipeline, x_train)\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    _save_model(\n",
    "        os.path.join(\"files/models/\", \"model.pkl.gz\"),\n",
    "        estimator,\n",
    "    )\n",
    "\n",
    "    y_test_pred = estimator.predict(x_test)\n",
    "    test_precision_metrics = calculate_metrics(\"test\", y_test, y_test_pred)\n",
    "    y_train_pred = estimator.predict(x_train)\n",
    "    train_precision_metrics = calculate_metrics(\"train\", y_train, y_train_pred)\n",
    "\n",
    "    test_confusion_metrics = calculate_confusion(\"test\", y_test, y_test_pred)\n",
    "    train_confusion_metrics = calculate_confusion(\"train\", y_train, y_train_pred)\n",
    "\n",
    "    os.makedirs(\"files/output/\", exist_ok=True)\n",
    "\n",
    "    with open(\"files/output/metrics.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(json.dumps(train_precision_metrics) + \"\\n\")\n",
    "        file.write(json.dumps(test_precision_metrics) + \"\\n\")\n",
    "        file.write(json.dumps(train_confusion_metrics) + \"\\n\")\n",
    "        file.write(json.dumps(test_confusion_metrics) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be470d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    _run_jobs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
